{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚ö†Ô∏è EDIT \"OPEN IN COLAB\" BADGE PRIOR TO DOING ASSIGNMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/BenjaminHerrera/MAT422/blob/main/HW_1.4.ipynb\">\n",
    "    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 1.4\n",
    "# Benjamin Herrera\n",
    "# 15 SEP 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚ö†Ô∏è Run these commands prior to running anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in c:\\users\\benhe\\miniconda3\\lib\\site-packages (1.12.0)\n",
      "Requirement already satisfied: numpy<1.29.0,>=1.22.4 in c:\\users\\benhe\\miniconda3\\lib\\site-packages (from scipy) (1.26.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\benhe\\miniconda3\\lib\\site-packages (3.8.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\benhe\\miniconda3\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\benhe\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\benhe\\miniconda3\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\benhe\\miniconda3\\lib\\site-packages (from matplotlib) (24.0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\benhe\\miniconda3\\lib\\site-packages (from matplotlib) (6.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\benhe\\miniconda3\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\benhe\\miniconda3\\lib\\site-packages (from matplotlib) (4.50.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\benhe\\miniconda3\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\benhe\\miniconda3\\lib\\site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in c:\\users\\benhe\\miniconda3\\lib\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\benhe\\miniconda3\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib) (3.18.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\benhe\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\benhe\\miniconda3\\lib\\site-packages (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install scipy\n",
    "!pip install matplotlib\n",
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìê Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.73603602 0.40202111 0.84023413 0.6033866 ]\n",
      " [0.94680652 0.15140284 0.16830036 0.41026334]\n",
      " [0.88225911 0.11779956 0.66367753 0.39480194]\n",
      " [0.72786678 0.58474827 0.6030089  0.98165908]\n",
      " [0.92598325 0.4581627  0.42531514 0.25542223]]\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "\n",
    "# Let's assume some random 5x4 matrix\n",
    "A = np.random.rand(5,4)\n",
    "\n",
    "# Show the matrix \n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.60380777 1.39305077 2.19607084 2.13190563]\n",
      " [1.39305077 0.75026413 0.98892587 1.04224507]\n",
      " [2.19607084 0.98892587 1.71929897 1.53863877]\n",
      " [2.13190563 1.04224507 1.53863877 1.71715505]]\n"
     ]
    }
   ],
   "source": [
    "# When we multiply the transpose of A against itself, we get a resulting matrix\n",
    "#   that is orthogonally diagonalized\n",
    "A_result = np.matmul(A.transpose(), A) \n",
    "\n",
    "# Show the matrix\n",
    "print(A_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues:\n",
      "[7.12280662 0.39930751 0.1846505  0.08376129]\n",
      "\n",
      "Eigenvectors:\n",
      "[[-0.69086219 -0.70866905  0.13742889  0.04013617]\n",
      " [-0.29933092  0.29730752  0.28017723 -0.86227023]\n",
      " [-0.46746492  0.29360447 -0.83379733 -0.00741474]\n",
      " [-0.46323534  0.56850051  0.45540755  0.5048011 ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now let's find the eigenvectors of A \n",
    "A_eigenvalues, A_eigenvectors = np.linalg.eig(A_result)\n",
    "\n",
    "# Show the eigenvalues and eigen vectors\n",
    "print(\"Eigenvalues:\")\n",
    "print(A_eigenvalues)\n",
    "print()\n",
    "print(\"Eigenvectors:\")\n",
    "print(A_eigenvectors)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.69086219  0.70866905  0.13742889  0.04013617]\n",
      " [-0.29933092 -0.29730752  0.28017723 -0.86227023]\n",
      " [-0.46746492 -0.29360447 -0.83379733 -0.00741474]\n",
      " [-0.46323534 -0.56850051  0.45540755  0.5048011 ]]\n"
     ]
    }
   ],
   "source": [
    "# Let's also get the orthonormal basis of the eigenvectors\n",
    "Q, _ = np.linalg.qr(A_eigenvectors)\n",
    "\n",
    "# Show the orthonormal basis \n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we conduct the following calculations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$||Av_i||^2 = (Av_i)^T Av_i = v_i^T A^T A v_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.1228066187857735\n",
      "0.3993075068665358\n",
      "0.18465049921850626\n",
      "0.08376128747760704\n"
     ]
    }
   ],
   "source": [
    "# Calculate the norm\n",
    "for i in range(len(A_eigenvectors)):\n",
    "    print(A_eigenvectors[i].T @ (A_eigenvalues[i] * A_eigenvectors[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which are the the eigenvalues of the matrix. This means that $||Av_i||^2$ are eigenvectors for $[i, n]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.6688586734381006, 0.6319078309900394, 0.42970978487638156, 0.2894154237037255]\n"
     ]
    }
   ],
   "source": [
    "# We define the singular values of A as the square root of the eigenvalues\n",
    "A_singular_values = [np.sqrt(i) for i in A_eigenvalues]\n",
    "\n",
    "# Show the singular values\n",
    "print(A_singular_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, this is just $\\{||Av_1||, \\dots, ||Av_n||\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# A cool thing is that the number of nonzero singular values equal to the \n",
    "#   dimensions of col(A)\n",
    "count = 0\n",
    "for i in A_singular_values:\n",
    "    if i >= 0:\n",
    "        count += 1\n",
    "        \n",
    "# Show the number of nonzero singular values\n",
    "print(count)\n",
    "\n",
    "# Show the dimension of col(A)\n",
    "rank = np.linalg.matrix_rank(A)\n",
    "col_space = Q[:, :rank]\n",
    "print(col_space.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U\n",
      "[[-0.4875218   0.29694212 -0.49337576  0.06478251 -0.65309376]\n",
      " [-0.36276007 -0.54329223  0.50975527 -0.39149436 -0.40015048]\n",
      " [-0.42636678 -0.27045594 -0.51040058 -0.44299959  0.53694279]\n",
      " [-0.53000692  0.62216859  0.48435206 -0.05554021  0.30711117]\n",
      " [-0.40991664 -0.39549811  0.04030206  0.80199877  0.17528063]]\n",
      "\n",
      "E\n",
      "[[2.66885867 0.         0.         0.        ]\n",
      " [0.         0.63190783 0.         0.        ]\n",
      " [0.         0.         0.42970978 0.        ]\n",
      " [0.         0.         0.         0.28941542]\n",
      " [0.         0.         0.         0.        ]]\n",
      "\n",
      "V\n",
      "[[-0.69086219 -0.29933092 -0.46746492 -0.46323534]\n",
      " [-0.70866905  0.29730752  0.29360447  0.56850051]\n",
      " [ 0.13742889  0.28017723 -0.83379733  0.45540755]\n",
      " [-0.04013617  0.86227023  0.00741474 -0.5048011 ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We can now conduct a SVD of A, simply with this line\n",
    "U, E, V = np.linalg.svd(A, full_matrices=True)\n",
    "\n",
    "# Show the m x m orthogonal matrix U\n",
    "print(\"U\")\n",
    "print(U)\n",
    "print()\n",
    "\n",
    "# Show the m x n matrix E\n",
    "diag_E = np.append(np.diag(E), [np.zeros(4)], axis=0)\n",
    "m, n = A.shape\n",
    "print(\"E\")\n",
    "print(np.append(np.diag(E), [np.zeros(4)] * (m - n), axis=0))\n",
    "print()\n",
    "\n",
    "# Show the n x n orthogonal matrix V\n",
    "print(\"V\")\n",
    "print(V)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed\n",
      "[[0.73603602 0.40202111 0.84023413 0.6033866 ]\n",
      " [0.94680652 0.15140284 0.16830036 0.41026334]\n",
      " [0.88225911 0.11779956 0.66367753 0.39480194]\n",
      " [0.72786678 0.58474827 0.6030089  0.98165908]\n",
      " [0.92598325 0.4581627  0.42531514 0.25542223]]\n",
      "\n",
      "Original\n",
      "[[0.73603602 0.40202111 0.84023413 0.6033866 ]\n",
      " [0.94680652 0.15140284 0.16830036 0.41026334]\n",
      " [0.88225911 0.11779956 0.66367753 0.39480194]\n",
      " [0.72786678 0.58474827 0.6030089  0.98165908]\n",
      " [0.92598325 0.4581627  0.42531514 0.25542223]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We can now construct the original matrix as such\n",
    "print(\"Reconstructed\")\n",
    "print(U @ diag_E @ V)\n",
    "print()\n",
    "\n",
    "# Show the original matrix\n",
    "print(\"Original\")\n",
    "print(A)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚õµ Low-Rank Matrix Approximations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same information (code and all), let's define the induced norm. This defined as the distance between two different matrixes. Chapter 1.4.2 definitions this as below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$||A||_2 = \\max_{0\\neq x \\isin \\Reals^{m}} \\frac{||Ax||}{||x||} = \\max_{x \\neq 0, ||x|| = 1} ||Ax|| = \\max_{x \\neq 0, ||X|| = 1} x^T A^T A x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$||A||_2 = \\sqrt{\\lambda_{max} A^T A}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the previous definitions of the singular values, $m \\times m$ U vectors, and the eigenvectors $v_j$, we can reconstruct $A$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.61035302 0.43415396 0.85245542 0.93367083 0.47768664]\n",
      " [0.12279571 0.77095948 0.89056949 0.89425462 0.51393668]\n",
      " [0.14078068 0.30877115 0.43290718 0.38346626 0.00283509]\n",
      " [0.17595007 0.25597648 0.20614041 0.49186853 0.97695307]]\n"
     ]
    }
   ],
   "source": [
    "# Let's define a new matrix in n x m format\n",
    "A = np.random.rand(4,5)\n",
    "A_result = A.transpose() @ A \n",
    "A_eigenvalues, A_eigenvectors = np.linalg.eig(A_result)\n",
    "Q, _ = np.linalg.qr(A_eigenvectors)\n",
    "A_singular_values = [np.sqrt(i) for i in A_eigenvalues]\n",
    "U, E, V = np.linalg.svd(A, full_matrices=True)\n",
    "n, m = A.shape\n",
    "\n",
    "# Print A\n",
    "print(A)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.61035302 0.43415396 0.85245542 0.93367083 0.47768664]\n",
      " [0.12279571 0.77095948 0.89056949 0.89425462 0.51393668]\n",
      " [0.14078068 0.30877115 0.43290718 0.38346626 0.00283509]\n",
      " [0.17595007 0.25597648 0.20614041 0.49186853 0.97695307]]\n"
     ]
    }
   ],
   "source": [
    "# Build a reconstruction of A with singular values, m x m U vectors, and \n",
    "#   eigenvectors of A\n",
    "A_reconstructed = np.zeros((n, m))\n",
    "for j in range(len(E)):\n",
    "    A_reconstructed += E[j] * np.outer(U[:, j], V[j, :])\n",
    "    \n",
    "# Print the resulting matrix\n",
    "print(A_reconstructed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YAY! It's the same! A cool thing is that we can create a \"resolution\" of the reconstruction by specifying if we want all of the of terms or part of it. Below is an example with half of the terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.35528545 0.60540485 0.857916   0.9036629  0.49657213]\n",
      " [0.36272779 0.61919077 0.87973855 0.92218113 0.49519425]\n",
      " [0.15452284 0.28321997 0.44258686 0.38560981 0.00352989]\n",
      " [0.18393282 0.24595117 0.20882012 0.49295825 0.9768514 ]]\n"
     ]
    }
   ],
   "source": [
    "# Build a reconstruction of A with singular values, m x m U vectors, and \n",
    "#   eigenvectors of A. \n",
    "# But this time, with half of the terms\n",
    "k = int(len(E) / 2)\n",
    "A_reconstructed_k = np.zeros((n, m))\n",
    "for j in range(k):\n",
    "    A_reconstructed_k += E[j] * np.outer(U[:, j], V[j, :])\n",
    "    \n",
    "# Print the resulting matrix\n",
    "print(A_reconstructed_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this idea, we can show that:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$||A - A_k||_2^2 = \\sigma_{k+1}^2$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1782551143511746\n",
      "0.17825511435117447\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate the above notion\n",
    "A_difference = A_reconstructed - A_reconstructed_k\n",
    "A_induced_norm = np.linalg.norm(A_difference, ord=2) # induced norm\n",
    "\n",
    "# Show similarities\n",
    "print(E[k] ** 2)\n",
    "print(A_induced_norm ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.35584265 0.09028438 0.75396553 0.30165654 0.27934636]\n",
      " [0.97983281 0.82202976 0.63337227 0.08999584 0.65890704]\n",
      " [0.52328962 0.37040286 0.27416364 0.56132893 0.58220969]\n",
      " [0.45954396 0.56458512 0.60519253 0.97137665 0.65447334]]\n"
     ]
    }
   ],
   "source": [
    "# To demonstrate the idea of principal component analysis, let's define a\n",
    "#   p x N matrix \n",
    "p = 4\n",
    "N = 5\n",
    "X = np.random.rand(p, N)\n",
    "\n",
    "# Show the content of X\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.66166397 0.6385032  0.25375207 0.42137771]\n"
     ]
    }
   ],
   "source": [
    "# Let's get an \"average\" of all of the vectors\n",
    "M = (1/N) * A.sum(axis=1)\n",
    "\n",
    "# Show the average of the vectors\n",
    "print(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.30582133  0.34132961  0.26953755  0.03816625]\n",
      " [-0.57137959  0.18352656  0.11665079  0.14320741]\n",
      " [ 0.09230155 -0.00513092  0.02041157  0.18381481]\n",
      " [-0.36000743 -0.54850736  0.30757686  0.54999893]\n",
      " [-0.38231761  0.02040384  0.32845762  0.23309563]]\n"
     ]
    }
   ],
   "source": [
    "# Now we subtract all of the original vectors in X with M\n",
    "B = np.vstack([i - M for i in X.T])\n",
    "\n",
    "# Show B \n",
    "print(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.07103493  0.06857264 -0.00436548  0.00669276  0.05532834]\n",
      " [ 0.06857264  0.0985681  -0.00624405  0.05491956  0.07347225]\n",
      " [-0.00436548 -0.00624405  0.01068761  0.0192403   0.00353939]\n",
      " [ 0.00669276  0.05491956  0.0192403   0.20689201  0.08891846]\n",
      " [ 0.05532834  0.07347225  0.00353939  0.08891846  0.07720026]]\n"
     ]
    }
   ],
   "source": [
    "# From this, we can make a covariance matrix of size p x p \n",
    "S = (1/(N-1)) * B @ B.T\n",
    "\n",
    "# Show the covariance matrix\n",
    "print(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PCA, we want to find some value k to find the top k number of components. This is done via:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\max \\frac{1}{N}\\sum_{i=1}^{N}\\sum_{j=1}^{k}<X_i \\cdot v_j>^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
