{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/BenjaminHerrera/MAT422/HW_1.2.ipynb\">\n",
    "    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 1.2\n",
    "# Benjamin Herrera\n",
    "# 1 SEP 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚èπÔ∏è Linear Spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can imagine the output of a linear combination (the multiplication of a set of vectors to a set of constants) as the linear subspace. In other words, the result of a linear combination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define a linear subspace, there are two things we have to keep in mind: (1) result of vector addition is in a subset of $V$ ($J$ where $J \\subseteq V$) and (2) multiplying vectors of $J$ to a scalar still results in vectors in $J$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, if we represent $j_i \\isin J$, then $j_1 + j_2 \\isin J$ and $\\alpha j_1 \\isin J$ where $\\alpha \\isin \\Reals$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get to the notion of a span, the scalability reach of a set of vectors across some space. For example, given some vectors $s_1$ to $S-i$ that are in $V$, the span of these vectors can be defined as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$span(s_1, \\dots, s_i) = \\{ \\sum_{k=1}^{i} \\alpha_k s_k \\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\alpha_i \\isin \\Reals$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This span can also be considered as a linear subspace $J$ (similar definition as above). Another interesting property is that this linear subspace also represents the span of $J$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also build an understanding of a columnspace with this knowledge of a span. Lets say for example we have an $n \\times m$ matrix. If we grab the columns of this matrix, we get $n$ number of vectors. These vectors are a set of vectors and therefore, can create a span. This span will still be in $\\Reals^n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, here's an issue. There can the possibility where we can have redundant vectors in our linear subspace. Sure, we call it a \"set\" of vectors, but matrixes are in essence a list of vectors. So how can we figure out if we have unique vectors you may ask? Well, we can just figure out if it's linear independent! How do we do that? Simple!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\forall i, j_i \\notin span({j_k k \\neq i})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $j_1, \\dots j_i \\isin J$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, the scaling of a vector cannot reach all of the possible reaches of other vectors, making that vector unique!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can also define a method via this definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sum_{k=1}^{n} \\alpha_k u_k = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\alpha_k$ is not zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! we now can figure out if all of the vectors in a matrix are unique to one another (cannot touch other vector's reaches in euclidean space). But, is there a minimum set of vectors that can still touch all of euclidean space? Yes! And it's called bases. And it's really simple to define:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Basis of J spans J\n",
    "2. Basis of J is linearly independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! Also note, people will refer to basis vectors as $e_i$. Here's a cool thing about basis vectors: (1) any subspace can have multiple bases and (2) all bases of the same subspace, must have the same cardinality of elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get onto the point of dimension! When we refer to the dimension of the column space of any subspace, we call that the rank of that subspace. Remember the two cool thins about basis vectors? Well, we can refer to the number of elements as the dimension of that subspace as $dim(J)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ûï Orthogonality "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can understand orthogonality as vectors being perpendicular of a plane. In other words, a line that is in normal direction to a slice in euclidean space. But how do we define orthogonality? Simple!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But before we work at defining it, let's define the norm and inner product of two vectors $a$ and $b$ as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$< a , b > = a \\cdot b = \\sum_{j}^{n} a_j b_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is also essentially, the dot product which gets a scalar value that represents the similarity of two vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To figure out if a list of vectors are orthonormal to each other, we say that: (1) for all vectors other than itself, a vector $a_i$ is orthonormal if it has a dot product of 0 across all vectors, and (2) the norm of itself is 1. If this is the case where the list of vectors are orthonormal, then the list is linearly independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use this understanding to build the best approximation theorem. This is where we solve for:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\min_{q^* \\isin Q} ||q^* - q||$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $q^* \\isin Q$ and is the vector to find for. This can be found via $q^* = <u_1, q> u_1$ where $u_1 \\isin U$. BY apply the Pythagorean theorem to this, we get $||q - \\alpha u_1||^2 \\geq ||q- q^*||^2$. We can also build the Cauchy-Schwarz Inequality which states that there his an upper bound of dot producting two vectors. It is simply stated as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$a, b \\isin J, |<a, b>| \\leq ||a|| ||b||$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Gram-Schmidt Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gram-Schmidt Process is an algorithm to find the orthonormal basis for a list of vectors. This is essentially finding the orthonormal basis of $span(j_1, \\dots, j_n)$. The way to do this is pretty simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$q_i = u_i - \\sum_{j=1}^{k-1} \\textrm{proj}_{u_j}(u_k)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $u_*, q_i \\isin U$. We can also normalize the the orthonormal vectors by getting their unit vectors as such:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$e_i = \\frac{q_i}{||q_i||}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $e_i$ is the unit version of the orthonormal vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Eigenvalues and Eigenvector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can see eigenvalues as the scale factor (with orientation) of one linear space to another. The the eigenvector complements this by providing a direction to scale from space A to space B. Assuming that $A\\isin \\Reals^{n \\times n}$, we can make this definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Ax = \\lambda x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\lambda$ is the eigenvalue of $A$. This assumes, thought, that $x$ is non-zero. Here, we call $x$ as ain eigenvector. One other property to note is that $A$ has at max $n$ number of unique eigenvalues. This correlates to the dimension of $A$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show that a matrix is diagonal, assume a matrix $B$ that is $diag(\\lambda_1, \\dots, \\lambda_n)$. We can reconstruct $A$ as $PDP^{-1}$ for some matrix $P$. We can then reconfigure this to be $AP=PD$ which can derived into $A p_i = \\lambda_i p_i$. This is now similar to the first definition of eigenvalues and eigenvectors. If this definition holds. We can state that $A$ is symmetric which to the following notions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* every symmetirc matrix is orthogonally diagonizable\n",
    "* $A$ has $d$ eigenvalues (this time duplicates are allowed)\n",
    "* If the eigenvalues of $A$ are multiples of $g$, then the dimension of the eigenspace of $A$ is $g$.\n",
    "* Eigenspaces are also orthogonal.\n",
    "* P are orthonormal eigenvectors of $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
