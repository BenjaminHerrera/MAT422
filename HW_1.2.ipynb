{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/BenjaminHerrera/MAT422/blob/main/HW_1.2.ipynb\">\n",
    "    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 1.2\n",
    "# Benjamin Herrera\n",
    "# 1 SEP 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ⚠️ Run these commands prior to running anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in c:\\users\\benhe\\miniconda3\\lib\\site-packages (1.12.0)\n",
      "Requirement already satisfied: numpy<1.29.0,>=1.22.4 in c:\\users\\benhe\\miniconda3\\lib\\site-packages (from scipy) (1.26.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\benhe\\miniconda3\\lib\\site-packages (3.8.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\benhe\\miniconda3\\lib\\site-packages (from matplotlib) (24.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\benhe\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in c:\\users\\benhe\\miniconda3\\lib\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\benhe\\miniconda3\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\benhe\\miniconda3\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\benhe\\miniconda3\\lib\\site-packages (from matplotlib) (6.4.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\benhe\\miniconda3\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\benhe\\miniconda3\\lib\\site-packages (from matplotlib) (4.50.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\benhe\\miniconda3\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\benhe\\miniconda3\\lib\\site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\benhe\\miniconda3\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib) (3.18.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\benhe\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\benhe\\miniconda3\\lib\\site-packages (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install scipy\n",
    "!pip install matplotlib\n",
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⏹️ Linear Spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can imagine the output of a linear combination (the multiplication of a set of vectors to a set of constants) as the linear subspace. In other words, the result of a linear combination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define a linear subspace, there are two things we have to keep in mind: (1) result of vector addition is in a subset of $V$ ($J$ where $J \\subseteq V$) and (2) multiplying vectors of $J$ to a scalar still results in vectors in $J$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, if we represent $j_i \\isin J$, then $j_1 + j_2 \\isin J$ and $\\alpha j_1 \\isin J$ where $\\alpha \\isin \\Reals$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get to the notion of a span, the scalability reach of a set of vectors across some space. For example, given some vectors $s_1$ to $S_i$ that are in $V$, the span of these vectors can be defined as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$span(s_1, \\dots, s_i) = \\{ \\sum_{k=1}^{i} \\alpha_k s_k \\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\alpha_i \\isin \\Reals$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This span can also be considered as a linear subspace $J$ (similar definition as above). Another interesting property is that this linear subspace also represents the span of $J$. Let's look a python implementation of this idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1404 -1404 -1404 -1404]\n"
     ]
    }
   ],
   "source": [
    "# Let's build an example list of four vectors\n",
    "import numpy as np\n",
    "\n",
    "v1 = np.array([1, 2, 3, 4])\n",
    "v2 = np.array([5, 6, 7, 8])\n",
    "v3 = np.array([9, 10, 11, 12])\n",
    "v4 = np.array([13, 14, 15, 16])\n",
    "\n",
    "# Now let's define some scalars\n",
    "a1 = -69\n",
    "a2 = 69\n",
    "a3 = 420\n",
    "a4 = -420\n",
    "\n",
    "# Now let's calculate the linear combination of all of them.\n",
    "# This will return a list of scalars, similar to the equation explained above\n",
    "lc = a1 * v1 + a2 * v2 + a3 * v3 + a4 * v4\n",
    "\n",
    "# Print the LC\n",
    "print(lc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above coding example, since the output of the linear combination are scalars with scalars in the set of reals, the linear combination is in the span of the three vectors. Had there been no scalars to get to what the linear combination is at this moment, then the linear combination is not in the span of the list of vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also build an understanding of a columnspace with this knowledge of a span. Lets say for example we have an $n \\times m$ matrix. If we grab the columns of this matrix, we get $n$ number of vectors. These vectors are a set of vectors and therefore, can create a span. This span will still be in $\\Reals^n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, here's an issue. There can the possibility where we can have redundant vectors in our linear subspace. Sure, we call it a \"set\" of vectors, but matrixes are in essence a list of vectors. So how can we figure out if we have unique vectors you may ask? Well, we can just figure out if it's linear independent! How do we do that? Simple!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\forall i, j_i \\notin span(\\{j_k : k \\neq i\\})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $j_1, \\dots j_i \\isin J$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, the scaling of a vector cannot reach all of the possible reaches of other vectors, making that vector unique!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can also define a method via this definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sum_{k=1}^{n} \\alpha_k u_k = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\alpha_k$ is not zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! we now can figure out if all of the vectors in a matrix are unique to one another (cannot touch other vector's reaches in euclidean space). But, is there a minimum set of vectors that can still touch all of euclidean space? Yes! And it's called bases. And it's really simple to define:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Basis of J spans J\n",
    "2. Basis of J is linearly independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! Also note, people will refer to basis vectors as $e_i$. Here's a cool thing about basis vectors: (1) any subspace can have multiple bases and (2) all bases of the same subspace, must have the same cardinality of elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get onto the point of dimension! When we refer to the dimension of the column space of any subspace, we call that the rank of that subspace. Remember the two cool thins about basis vectors? Well, we can refer to the number of elements as the dimension of that subspace as $dim(J)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see all of this in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# We'll define a function that uses the rank of a matrix and the number of\n",
    "#   columns to see if the matrix is linearly independent.\n",
    "def is_linearly_independent(matrix):\n",
    "    rank = np.linalg.matrix_rank(matrix)\n",
    "    return rank == matrix.shape[0]\n",
    "\n",
    "\n",
    "# Here is an example of a linearly DEPENDENT matrix\n",
    "m1 = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]])\n",
    "print(is_linearly_independent(m1))\n",
    "\n",
    "# Here is an example of a linearly INDEPENDENT matrix\n",
    "m2 = np.array([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]])\n",
    "print(is_linearly_independent(m2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, we see that we are checking if the rank of the matrix matches to the number of columns in the matrix. If so, it is linear INDEPENDENT. If not, linearly DEPENDENT. We can see this to be the case in the non-identity matrix and the identity matrix. The identity matrix has linearly independency, because there is no way that the vectors in that matrix can reach the other vectors' reach if when the all scale. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ➕ Orthogonality "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can understand orthogonality as vectors being perpendicular of a plane. In other words, a line that is in normal direction to a slice in euclidean space. But how do we define orthogonality? Simple!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But before we work at defining it, let's define the norm and inner product of two vectors $a$ and $b$ as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$< a , b > = a \\cdot b = \\sum_{j}^{n} a_j b_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is also essentially, the dot product which gets a scalar value that represents the similarity of two vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To figure out if a list of vectors are orthonormal to each other, we say that: (1) for all vectors other than itself, a vector $a_i$ is orthonormal if it has a dot product of 0 across all vectors, and (2) the norm of itself is 1. If this is the case where the list of vectors are orthonormal, then the list is linearly independent. Below is a simple example of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "================\n",
      "0.9999999999999999\n",
      "0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "# Let's use two vectors\n",
    "v1 = np.array([(-23*np.sqrt(20129))/20129, (140*np.sqrt(20129))/20129])\n",
    "v2 = np.array([(140*np.sqrt(20129))/20129, (23*np.sqrt(20129))/20129])\n",
    "\n",
    "# Let's see the dot product between the two\n",
    "print(np.dot(v1, v2))\n",
    "\n",
    "# And here's the norm of each vector:\n",
    "print(\"================\")\n",
    "print(np.linalg.norm(v1))\n",
    "print(np.linalg.norm(v2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we can see that the dot product between the two vectors are 0. And that the norms of each of them are almost to 1 (we'll round them and treat them as the value of 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use this understanding to build the best approximation theorem. This is where we solve for:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\min_{q^* \\isin Q} ||q^* - q||$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $q^* \\isin Q$ and is the vector to find for. This can be found via $q^* = <u_1, q> u_1$ where $u_1 \\isin U$. BY apply the Pythagorean theorem to this, we get $||q - \\alpha u_1||^2 \\geq ||q- q^*||^2$. We can also build the Cauchy-Schwarz Inequality which states that there his an upper bound of dot producting two vectors. It is simply stated as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$a, b \\isin J, |<a, b>| \\leq ||a|| ||b||$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚙️ Gram-Schmidt Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gram-Schmidt Process is an algorithm to find the orthonormal basis for a list of vectors. This is essentially finding the orthonormal basis of $span(j_1, \\dots, j_n)$. The way to do this is pretty simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$q_i = u_i - \\sum_{j=1}^{k-1} \\textrm{proj}_{u_j}(u_k)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $u_*, q_i \\isin U$. We can also normalize the the orthonormal vectors by getting their unit vectors as such:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$e_i = \\frac{q_i}{||q_i||}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $e_i$ is the unit version of the orthonormal vector. Here's an example of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v1_proc [0.70710678 0.70710678]\n",
      "v2_proc [nan nan]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BenHe\\AppData\\Local\\Temp\\ipykernel_13280\\3608982702.py:13: RuntimeWarning: invalid value encountered in divide\n",
      "  v2_proc = v2_proc / np.linalg.norm(v2_proc)\n"
     ]
    }
   ],
   "source": [
    "# Define two vectors for example\n",
    "v1 = np.array([69, 69])\n",
    "v2 = np.array([420, 420])\n",
    "\n",
    "# Normalize v1\n",
    "v1_proc = v1 / np.linalg.norm(v1)\n",
    "\n",
    "# Subtract proj of v2 on v1_proc to get v2_proc\n",
    "proj = (np.dot(v2, v1_proc) / np.dot(v1_proc, v1_proc)) * v1_proc\n",
    "v2_proc = v2 - proj\n",
    "\n",
    "# Normalize v2\n",
    "v2_proc = v2_proc / np.linalg.norm(v2_proc)\n",
    "\n",
    "# Print out the transformed vectors\n",
    "print(\"v1_proc\", v1_proc)\n",
    "print(\"v2_proc\", v2_proc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📦 Eigenvalues and Eigenvector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can see eigenvalues as the scale factor (with orientation) of one linear space to another. The the eigenvector complements this by providing a direction to scale from space A to space B. Assuming that $A\\isin \\Reals^{n \\times n}$, we can make this definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Ax = \\lambda x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\lambda$ is the eigenvalue of $A$. This assumes, thought, that $x$ is non-zero. Here, we call $x$ as ain eigenvector. One other property to note is that $A$ has at max $n$ number of unique eigenvalues. This correlates to the dimension of $A$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show that a matrix is diagonal, assume a matrix $B$ that is $diag(\\lambda_1, \\dots, \\lambda_n)$. We can reconstruct $A$ as $PDP^{-1}$ for some matrix $P$. We can then reconfigure this to be $AP=PD$ which can derived into $A p_i = \\lambda_i p_i$. This is now similar to the first definition of eigenvalues and eigenvectors. If this definition holds. We can state that $A$ is symmetric which to the following notions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* every symmetirc matrix is orthogonally diagonizable\n",
    "* $A$ has $d$ eigenvalues (this time duplicates are allowed)\n",
    "* If the eigenvalues of $A$ are multiples of $g$, then the dimension of the eigenspace of $A$ is $g$.\n",
    "* Eigenspaces are also orthogonal.\n",
    "* P are orthonormal eigenvectors of $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of eigenvalues and eigenvectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix [[ 69 420]\n",
      " [420  69]]\n",
      "eigenvalues [ 489. -351.]\n",
      "eigenvectors [[ 0.70710678 -0.70710678]\n",
      " [ 0.70710678  0.70710678]]\n"
     ]
    }
   ],
   "source": [
    "# An example of a matrix\n",
    "matrix = np.array([[69, 420], [420, 69]])\n",
    "\n",
    "# Using numpy, we can extract the eigenvalues and the eigenvectors\n",
    "values, vectors = np.linalg.eig(matrix)\n",
    "\n",
    "# Display the eigenvectors and eigenvalues\n",
    "print(\"matrix\", matrix)\n",
    "print(\"eigenvalues\", values)\n",
    "print(\"eigenvectors\", vectors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
