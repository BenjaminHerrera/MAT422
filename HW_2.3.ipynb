{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚ö†Ô∏è EDIT \"OPEN IN COLAB\" BADGE PRIOR TO DOING ASSIGNMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/BenjaminHerrera/MAT422/blob/main/HW_2.3.ipynb\">\n",
    "    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 2.3\n",
    "# Benjamin Herrera\n",
    "# 29 SEP 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚ö†Ô∏è Run these commands prior to running anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in c:\\users\\benhe\\miniconda3\\lib\\site-packages (1.12.0)\n",
      "Requirement already satisfied: numpy<1.29.0,>=1.22.4 in c:\\users\\benhe\\miniconda3\\lib\\site-packages (from scipy) (1.26.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\benhe\\miniconda3\\lib\\site-packages (3.8.3)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\benhe\\miniconda3\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\benhe\\miniconda3\\lib\\site-packages (from matplotlib) (4.50.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\benhe\\miniconda3\\lib\\site-packages (from matplotlib) (24.0)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in c:\\users\\benhe\\miniconda3\\lib\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\benhe\\miniconda3\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\benhe\\miniconda3\\lib\\site-packages (from matplotlib) (6.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\benhe\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\benhe\\miniconda3\\lib\\site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\benhe\\miniconda3\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\benhe\\miniconda3\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\benhe\\miniconda3\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib) (3.18.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\benhe\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\benhe\\miniconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\benhe\\miniconda3\\lib\\site-packages (1.4.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\benhe\\miniconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\benhe\\appdata\\roaming\\python\\python39\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\benhe\\miniconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\benhe\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scipy\n",
    "!pip install matplotlib\n",
    "!pip install numpy\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí™ Joint Probability Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A joint probability is the likelihood of 2+ unique, different, or the same events happening at the same time. This feature shows a distribution for each random variable occurring. For example, we flip multiple coins and we have a random variable $X$ that defines the number of times the dice has $3$ in two rolls. And a random variable $Y$ that represents the value of two rolls added up. A join probability distribution would combine the two distributions together. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show the probability mass function for the given example, let's define the intake of two arguments:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(x, y) = P(X=x \\textrm{ and } Y = y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $x$ and $y$ are possible values of $X$ and $Y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can isolate the probability mass function for a given variable. For example if we just want to focus on $x$, we can define the PMF as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p_x(x) = \\sum_{y \\isin Y:p(x, y) > 0} p(x, y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for y:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p_y(y) = \\sum_{x \\isin X:p(x, y) > 0} p(x, y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a code example of the PMF for discrete joint probability distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joint PMF Matrix:\n",
      "Y    1    2\n",
      "X          \n",
      "1  0.5  0.0\n",
      "2  0.0  0.5\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Define X and Y\n",
    "data = pd.DataFrame({\n",
    "    'X': [1, 2],\n",
    "    'Y': [1, 2]\n",
    "})\n",
    "\n",
    "# Define what the PMF matrix\n",
    "joint_freq = pd.crosstab(data['X'], data['Y'])\n",
    "\n",
    "# Convert frequency to joint probability by dividing by total number of samples\n",
    "joint_pmf = joint_freq / joint_freq.sum().sum()\n",
    "\n",
    "# Show the PMF matrix of the two variables\n",
    "print(\"Joint PMF Matrix:\")\n",
    "print(joint_pmf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a code example for the marginal distribution of X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marginal Distribution of X:\n",
      "Px(1) = 0.5000\n",
      "Px(2) = 0.5000\n"
     ]
    }
   ],
   "source": [
    "# Calculate the marginal distribution of X\n",
    "marginal_p_x = joint_pmf.sum(axis=1)\n",
    "\n",
    "# Print the marginal distribution of X\n",
    "x_values = [1, 2]\n",
    "marginal_distribution_x = dict(zip(x_values, marginal_p_x))\n",
    "print(\"Marginal Distribution of X:\")\n",
    "for x, prob in marginal_distribution_x.items():\n",
    "    print(f\"Px({x}) = {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For continuous random variables, we can utilize integrals nested inside one another for every random variable we use. This is then called a join density function. To define this, we describe $A = \\{(x, y) : a_x \\leq x \\leq b_x, a_y \\leq y \\leq b_y\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P((X, Y) \\isin A) = \\int\\int_A f(x,y) dx dy = \\int_{a_x}^{b_x}\\int_{a_y}^{b_y} f(x, y) dy dx$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a more smoother distribution over two random variables. The above example works for two variables, but anymore variables included, then more integrals need to be placed. The ranges for these bounds are from negative infinity to infinity, usually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if the occurrences of two (or more) variables are independent? In other words, what if $X$ didn't depend on $Y$ to occur? We can easily still show the probability functions for two discrete variables as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p_{X, Y}(x, y) = p_x(x) \\cdot p_y(y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and for continuous variables:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f_{X, Y}(x, y) = f_x(x) \\cdot f_y(y)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25\n"
     ]
    }
   ],
   "source": [
    "# Using the example from the coins, if the occurrences are independent, then\n",
    "# P_{X, Y}(1, 1) is:\n",
    "print(joint_pmf.iloc[1][2] * joint_pmf.iloc[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§µ Correlation and Independence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If two events occur in correlation with each other, we can use joint distributions to predict future occurrences, called a covariance. This is defined via this equation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Cov(X, Y) = \\mathop{\\mathbb{E}}[(X - \\mu_x) (Y - \\mu_y)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For discrete events, we can further specify it to:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sum_x \\sum_y (x - \\mu_x)(y-\\mu_y)p(x, y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For continuous ones, we can define:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\int_{- \\inf}^{inf} \\int_{- \\inf}^{inf} (x-\\mu_x)(y-\\mu_y)f(x,y) dx dy$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, we can define a correlation coefficient between X and Y via the following definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Corr(X, Y) = \\frac{Cov(X, Y)}{\\sigma_x \\cdot \\sigma_y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the value of Corr is 0, X and Y are independent but doesn't deduct to it. If the value falls [-1, 1], excluding 0, then there is a linear function $y = ax + b$ for some scalar $a \\neq 0$ and $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we apply this correlation coefficient to a sample, we can represent its correlation coefficient (Pearson CC) as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$PCC_{xy} = \\frac{\\sum (x_i - \\bar{x}) (y_i - \\bar{y})}{\\sqrt{\\sum (x_i - \\bar{x})^2} \\sqrt{\\sum (y_i - \\bar{y})^2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables with a bar on top of them are the means for that event sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé≤ Random Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike random variables, a random sample is a selection of an event (set of actions) over all of the possible set of actions. Thus, this set of random variables must satisfy two conditions. The first is that all of the random variables are independent. The second is that all of them have the same probability distribution. This also means that the mean and the variance of the random samples are the same as the population. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above notions plays in well with Central Limit Theorem. Imagine this, we try to figure out the nation's stance on voting Kamala Harris and Donald Trump for President. Now we can't go about and knock on every door, asking what their stance is. That type of sampling would take forever! So, we sample $n$ number of people for a city and see what they say [1]. Because of the facts of the random sample, the sampled statistics should correlate to the population as a whole."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] We assume that the stratification sampling strategy equalizes all aspects of the demographics in that city."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
